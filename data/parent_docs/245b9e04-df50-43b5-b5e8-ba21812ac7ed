{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "We used the Adam optimizer [ 20] with \u03b21= 0.9,\u03b22= 0.98and\u03b5= 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nlrate =d\u22120.5\nmodel\u00b7min(step_num\u22120.5, step _num\u00b7warmup _steps\u22121.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup _steps = 4000 .\n5.4 Regularization\nWe employ three types of regularization during training:", "metadata": {"source": "uploaded_docs\\Attention_is_all_you_need.pdf", "page": 6, "doc_id": "fea1247f-3bb3-4b76-992b-ec3dd16d6c2e"}, "type": "Document"}}