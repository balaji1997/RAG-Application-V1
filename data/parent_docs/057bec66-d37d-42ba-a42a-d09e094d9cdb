{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "those under Annex III use cases (below), except if:\n\nthe AI system performs a narrow procedural task;\n\nimproves the result of a previously completed human activity;\n\ndetects decision-making patterns or deviations from prior decision-making patterns and is not meant to replace or influence the previously completed human assessment without proper human review; or\n\nperforms a preparatory task to an assessment relevant for the purpose of the use cases listed in Annex III.\n\nAI systems are always considered high-risk if it profiles individuals, i.e. automated processing of personal\ndata to assess various aspects of a person\u2019s life, such as work performance, economic situation, health,\npreferences, interests, reliability, behaviour, location or movement.\n\nProviders whose AI system falls under the use cases in Annex III but believes it is not high-risk must document such an\nassessment before placing it on the market or putting it into service.\n\nRequirements for providers of high-risk AI systems (Art. 8\u201317)\n\nHigh risk AI providers must:\n\nEstablish a risk management system throughout the high risk AI system\u2019s lifecycle;\n\nConduct data governance, ensuring that training, validation and testing datasets are relevant, sufficiently representative and, to the best extent possible, free of errors and complete according to the intended purpose.\n\nDraw up technical documentation to demonstrate compliance and provide authorities with the information to assess that compliance.", "metadata": {"source": "uploaded_docs\\EU AI Act Doc.docx", "doc_id": "db37fc09-4b70-4ba7-b3e1-63005ba07907"}, "type": "Document"}}