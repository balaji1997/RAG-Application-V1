{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "Similarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by\u221admodel.", "metadata": {"source": "uploaded_docs\\Attention_is_all_you_need.pdf", "page": 4, "doc_id": "48c55aa4-b49c-4e47-a9f1-6c9067fe18a4"}, "type": "Document"}}